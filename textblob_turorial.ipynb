{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-06 02:07:08--  http://nltk.org/data.html\n",
      "Resolving nltk.org (nltk.org)... 15.197.142.173, 3.33.152.147\n",
      "Connecting to nltk.org (nltk.org)|15.197.142.173|:80... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2024-05-06 02:07:08 ERROR 404: Not Found.\n",
      "\n",
      "Requirement already satisfied: textblob in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk>=3.8->textblob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk>=3.8->textblob) (2024.4.28)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk>=3.8->textblob) (4.66.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# adopted from https://textblob.readthedocs.io/en/dev/quickstart.html#quickstart\n",
    "# create a TextBlob\n",
    "\n",
    "!wget http://nltk.org/data.html\n",
    "%pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "wiki = TextBlob(\"Python is a high-level, general-purpose programming language.\") #wiki에는 영어본\n",
    "wiki2 = TextBlob(\"Python은 고급 범용 프로그래밍 언어입니다.\") #wiki2는 번역본\n",
    "\n",
    "# 오류 발생시에는 python -m textblob.download_corpora 를 통해서 terminal에 실행 후 다음 code를 출력할 것.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech taging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('high-level', 'JJ'),\n",
       " ('general-purpose', 'JJ'),\n",
       " ('programming', 'NN'),\n",
       " ('language', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# console에서 python -m textblob.download_corpora 먼저 실행\n",
    "# Part-of-speech Tagging¶\n",
    "\n",
    "# 기존의 wiki에 저장된 값을 나눠서 품사별로 분류\n",
    "\n",
    "# NNP : 명사\n",
    "# VBZ : 동사\n",
    "# DT : determiner(한정사, (a/the/none))\n",
    "# JJ : 형용사\n",
    "# NN : 명사\n",
    "\n",
    "wiki.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python은', 'NNP'),\n",
       " ('고급', 'NNP'),\n",
       " ('범용', 'NNP'),\n",
       " ('프로그래밍', 'NNP'),\n",
       " ('언어입니다', 'NNP')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존의 wiki2에 저장된 한국어 문장을 품사별로 분류\n",
    "\n",
    "# NNP : 명사\n",
    "# VBZ : 동사\n",
    "# DT : determiner(한정사, (a/the/none))\n",
    "# JJ : 형용사\n",
    "# NN : 명사\n",
    "\n",
    "# 하지만 분류가 틀렸는데 인식 즉 분류가 불가능하기에 NNP로 일괄 출력하는데 -> 영어를 사용해야 함을 확인 가능함.\n",
    "\n",
    "wiki2.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from konlpy) (1.5.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from konlpy) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.6 in /home/codespace/.local/lib/python3.10/site-packages (from konlpy) (1.26.4)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[('Python', 'Alpha'), ('은', 'Noun'), ('고급', 'Noun'), ('범용', 'Noun'), ('프로그래밍', 'Noun'), ('언어', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# KoNLPy는 한국어 자연어 처리를 위한 Python 라이브러리\n",
    "# 상단의 wiki2.tags를 통해 한국어 분류가 불가능하기 때문에 KoNLPy를 사용한다.\n",
    "# POS 태깅을 포함해 한국어 텍스트 분석에 적합한 여러 도구에 대한 액세스를 제공\n",
    "# KoNLPy는 Mecab, Komoran, Hannanum, Kkma, and Twitter (Okt)등 다양한 한국어 NLP 도구를 지원\n",
    "\n",
    "%pip install konlpy\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "wiki2 = \"Python은 고급 범용 프로그래밍 언어입니다.\"\n",
    "pos_tags = okt.pos(wiki2)\n",
    "print(pos_tags)\n",
    "\n",
    "# 하지만 KoNLPy도 인식 및 품사 분류를 정확하게는 하지 못함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 한국어는 아직은 NLP를 통한 분류가 쉽지 않음을 확인 가능하며 우선은 영어 즉 wiki에 저장된 영어 문장을 활용하여 NLP 처리 과정을 확인해 보겠다.\n",
    "\n",
    "하단의 코드를 통해서는 중요한 과정이 왜? 이런 결과가 나왔는지에 대한 이해를 중심으로 결과의 도출 이유, 맞은 이유, 틀린 이유, 개선의 가능성에 대한 분석이 가능해야 함에 따라 설명을 할 줄 알아야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['python'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noun Phrase Extraction¶\n",
    "\n",
    "wiki.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.39166666666666666, subjectivity=0.4357142857142857)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment(polarity, subjectivity) #긍정/부정[-1 ~ 1], 객관/주관[0 ~ 1]에 대한 수치를 값으로 판단함\n",
    "# The polarity score is a float within the range [-1.0, 1.0].\n",
    "# The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective # 0은 객관적, 1은 주관적\n",
    "\n",
    "testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
    "testimonial.sentiment\n",
    "\n",
    "# 긍정의 문장을 넣으면 polarity의 값이 높아야 할 것이고, subjectivity의 값이 높으면 주관적임을 의미함.\n",
    "# polarity : 긍/부정 # subjectivity : 객/주관적\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Beautiful', 'is', 'better', 'than', 'ugly', 'Explicit', 'is', 'better', 'than', 'implicit', 'Simple', 'is', 'better', 'than', 'complex'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "# 단어를 문장에서 하나씩 분리하여 추출하는 방법\n",
    "\n",
    "zen = TextBlob(\n",
    "    \"Beautiful is better than ugly. \"\n",
    "    \"Explicit is better than implicit. \"\n",
    "    \"Simple is better than complex.\"\n",
    ")\n",
    "zen.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"Beautiful is better than ugly.\"),\n",
       " Sentence(\"Explicit is better than implicit.\"),\n",
       " Sentence(\"Simple is better than complex.\")]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장을 뽑는 방법 (sentences)\n",
    "\n",
    "zen.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words Inflection and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Use', '4', 'spaces', 'per', 'indentation', 'level'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words Inflection and Lemmatization (과거, 현재, 미래 진행 형을 모두 기본형으로 바꾸는 방법임)\n",
    "\n",
    "sentence = TextBlob(\"Use 4 spaces per indentation level.\")\n",
    "sentence.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'space'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# singularize() : 단어의 단수화\n",
    "\n",
    "sentence.words[2].singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'levels'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pluuralize() : 단어의 복수화\n",
    "\n",
    "sentence.words[-1].pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'octopus'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words can be lemmatized by calling the lemmatize method.\n",
    "# 복수형 단어을 주고 단수형으로 찾아라\n",
    "\n",
    "from textblob import Word\n",
    "w = Word(\"octopi\")\n",
    "w.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 과거형 단어를 주고 현재형(기본형)으로 바꿔라\n",
    "\n",
    "w = Word(\"went\")\n",
    "w.lemmatize(\"v\")  # Pass in WordNet part of speech (verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('octopus.n.01'), Synset('octopus.n.02')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordNet Integration¶\n",
    "# You can access the synsets for a Word via the synsets property or the get_synsets method, optionally passing in a part of speech.\n",
    "\n",
    "from textblob import Word\n",
    "from textblob.wordnet import VERB\n",
    "word = Word(\"octopus\")\n",
    "word.synsets\n",
    "\n",
    "# synsets를 통해 의미의 종류를 출력 (의미가 많을 경우 많이 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chop.v.05'),\n",
       " Synset('hack.v.02'),\n",
       " Synset('hack.v.03'),\n",
       " Synset('hack.v.04'),\n",
       " Synset('hack.v.05'),\n",
       " Synset('hack.v.06'),\n",
       " Synset('hack.v.07'),\n",
       " Synset('hack.v.08')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어 단어 \"hack\"의 의미의 종류를 출력 (8개)\n",
    "\n",
    "Word(\"hack\").get_synsets(pos=VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tentacles of octopus prepared as food',\n",
       " 'bottom-living cephalopod having a soft oval body with eight long tentacles']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can access the definitions for each synset via the definitions property or the define() method, which can also take an optional part-of-speech argument.\n",
    "# 정의 속성 또는 definite() 메서드를 통해 각 synset에 대한 정의에 액세스할 수 있으며, 이 메서드는 선택적인 품사 인수를 취할 수도 있습니다.\n",
    "\n",
    "Word(\"octopus\").definitions\n",
    "\n",
    "# 즉 definitions()를 통해 위에서 synsets를 통해서 확인 가능한 의미의 종류를 모두 세부적으로 보여줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one who works hard at boring tasks',\n",
       " 'a politician who belongs to a small clique that controls a political party for private rather than public ends',\n",
       " 'a mediocre and disdained writer',\n",
       " 'a tool (as a hoe or pick or mattock) used for breaking up the surface of the soil',\n",
       " 'a car driven by a person whose job is to take passengers where they want to go in exchange for money',\n",
       " 'an old or over-worked horse',\n",
       " 'a horse kept for hire',\n",
       " 'a saddle horse used for transportation rather than sport etc.',\n",
       " 'cut with a hacking tool',\n",
       " 'be able to manage or manage successfully',\n",
       " 'cut away',\n",
       " 'kick on the arms',\n",
       " 'kick on the shins',\n",
       " 'fix a computer program piecemeal until it works',\n",
       " 'significantly cut up a manuscript',\n",
       " 'cough spasmodically']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 즉 definitions()를 통해 위에서 synsets를 통해서 확인 가능한 의미의 종류를 모두 세부적으로 보여줌\n",
    "\n",
    "Word(\"hack\").definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 임베딩의 원조로 볼 수 있는 부분인데, 단어에 해당하는 특정 의미간의 가까운 정도(친밀성)을 확인하는 방법이다.\n",
    "\n",
    "즉 특정 단어의 특정 의미간의 관계를 수치값으로 확인하는 것으로 (0 ~ 1)로 관계 없음 ~ 관계 있음으로 수치를 통해 확인 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111111111111"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob.wordnet import Synset\n",
    "octopus = Synset(\"octopus.n.02\")\n",
    "shrimp = Synset(\"shrimp.n.03\")\n",
    "octopus.path_similarity(shrimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090909090909091"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob.wordnet import Synset\n",
    "octopus = Synset(\"octopus.n.02\")\n",
    "hack = Synset(\"hack.n.02\")\n",
    "octopus.path_similarity(hack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word(\"dog\").get_synsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07692307692307693"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog = Synset(\"doc.n.01\")\n",
    "octopus.path_similarity(dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['cat', 'dog', 'octopus'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A WordList is a Python list with additional methods.\n",
    "\n",
    "animals = TextBlob(\"cat dog octopus\")\n",
    "animals.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['cats', 'dogs', 'octopodes'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals.words.pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맞춤법, 문법이 틀린 것을 고쳐주는 기능 (correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "# Spelling Correction\n",
    "# Use the correct() method to attempt spelling correction.\n",
    "\n",
    "b = TextBlob(\"I havv goood speling!\")\n",
    "print(b.correct())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fallibility', 1.0)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word objects have a spellcheck() Word.spellcheck() method that returns a list of (word, confidence) tuples with spelling suggestions.\n",
    "\n",
    "from textblob import Word\n",
    "w = Word(\"falibility\")\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 특정 단어의 빈도수를 확인 하는 방법 word_counts['단어']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Word and Noun Phrase Frequencies¶\n",
    "# The first is through the word_counts dictionary.\n",
    "\n",
    "\n",
    "monty = TextBlob(\"We are no longer the Knights who say Ni. \"\n",
    "                    \"We are now the Knights who say Ekki ekki ekki PTANG.\")\n",
    "monty.word_counts['ekki']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second way is to use the count() method.\n",
    "\n",
    "monty.words.count('ekki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can specify whether or not the search should be case-sensitive (default is False).\n",
    "\n",
    "monty.words.count('ekki', case_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each of these methods can also be used with noun phrases.\n",
    "\n",
    "wiki.noun_phrases.count('python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing\n",
    "\n",
    "파싱(Parsing)은 문장을 문법 규칙과 여러 종류의 규칙을 이용하여 구조화 한 것을 의미한다. \n",
    "\n",
    "이 파싱을 하는 프로그램을 파서(Parser) 혹은 구문 분석기(Syntax Analyzer)라고 하고 문장을 수식과 결합 관계의 트리 구조로 표현한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And/CC/O/O now/RB/B-ADVP/O for/IN/B-PP/B-PNP something/NN/B-NP/I-PNP completely/RB/B-ADJP/O different/JJ/I-ADJP/O ././O/O\n"
     ]
    }
   ],
   "source": [
    "# Parsing\n",
    "# By default, TextBlob uses pattern’s parser\n",
    "\n",
    "b = TextBlob(\"And now for something completely different.\")\n",
    "print(b.parse())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams\n",
    "\n",
    "연속적으로 있는 단어를 찾아주는 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is', 'better']),\n",
       " WordList(['is', 'better', 'than']),\n",
       " WordList(['better', 'than', 'never'])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-grams\n",
    "# The TextBlob.ngrams() method returns a list of tuples of n successive words.\n",
    "\n",
    "blob = TextBlob(\"Now is better than never.\")\n",
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
